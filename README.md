**MEARN: Enhancing B-cell Epitope Prediction with ESM-Derived Protein Embeddings and an Attention–Residual Neural Network**

B-cell epitopes are key targets in vaccine design and therapeutic antibody development, yet accurate prediction remains challenging due to severe class imbalance, high sequence heterogeneity, and limited experimentally validated data. These issues often lead to biased learning and poor generalization in conventional deep learning models.
To address these challenges, we propose MEARN, an Attention–Residual Neural Network that leverages semantically rich protein embeddings from the ESM protein language model. MEARN integrates multi-head attention with residual MLP blocks and Layer Normalization to capture latent feature interactions, stabilize training, and enhance representation learning under imbalanced data conditions.
Experimental results demonstrate that MEARN achieves strong and balanced performance in B-cell epitope prediction (ACC = 0.9287, AUC = 0.8092, BAC = 0.7345, MCC = 0.4983, PR-AUC = 0.4948), confirming the effectiveness of combining ESM embeddings with an attention–residual architecture.
Overall, MEARN provides a robust and extensible framework for biological sequence analysis and offers valuable methodological insights for future research in bioinformatics and computational immunology

**Requirement**
⮚	Google Colab.
⮚	Library: Keras, Tensorflow, Sklearn, Numpy, Pandas…

**Dataset**
In order to prepare training and testing datasets for the work, we adopted the dataset from Israeli and Louzoun [25], which originally comprised 11,725 proteins, with 10% reserved as an external test set. The original publication provided both linear and conformational epitope data; however, we focused exclusively on the linear epitope dataset. Based on this dataset, residue-level samples were generated by applying a sliding window of nine amino acids centered on each position. Residues located within experimentally validated epitopes were labeled as positive, while all other residues were labeled as negative. 
After preprocessing, the final dataset comprised 89,332 residues in the training set (9,773 positives and 79,559 negatives) and 23,837 residues in the test set (2,267 positives and 21,570 negatives)
Model learning: We use Google colab pro buid this model
⮚	Train model: main.py; built_model.py
⮚	Cross validation: cross_validation.py
⮚	Independent test and predict: test_independent.py

**Contact**
Please feel free to contact us if you need any help: nvnui@ictu.edu.vn
