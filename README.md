B-cell epitopes play a central role in humoral immune responses and are essential targets for vaccine design and therapeutic antibody development. However, predicting B-cell epitopes remains highly challenging because available training datasets are often severely imbalanced, epitopes are heterogeneous and diverse in their sequence contexts, and the number of experimentally validated epitopes remains limited. These factors reduce the quality and coverage of training data and cause conventional deep learning models to suffer from bias and poor generalization. To address these limitations, we propose a novel Attention–Residual Network that leverages semantically rich embeddings from the ESM protein language model. The key innovation of our architecture lies in integrating multi-head attention, which captures hidden relationships across embedding dimensions, with residual MLP blocks and LayerNorm to stabilize training and enhance representation learning under class imbalance. This design enables more discriminative and generalizable feature learning compared with traditional approaches. Experimental results demonstrate that our model achieves strong overall performance (ACC = 0.9287, AUC = 0.8092, BAC = 0.7345, MCC = 0.4983, PR-AUC = 0.4948, sensitivity = 0.5082, specificity = 0.9623). These findings confirm that combining ESM embeddings with an attention–residual architecture provides a powerful, balanced, and reliable solution for B-cell epitope prediction, offering potential applications in next-generation vaccine design.
